## Literature Review
Excel file can be found [here](https://docs.google.com/spreadsheets/d/1M98qnhK85EqXm91nIv08I58I3GaLlqv6hfQQyXoKp3U/edit#gid=1016976313-)

|                                          Paper                                          |                                                                                                                                                                                                                                                       What's new?                                                                                                                                                                                                                                                       | Methods                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        What have we learned?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                               Open problems?                                                                                                                                                              |
|:---------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|           3D MRI brain tumor segmentation using <br>autoencoder regularization          |                                                                                                                                           Due to a limited training dataset size, <br>a variational auto-encoder branch is added <br>to reconstruct the input image itself in order to <br>regularize the shared decoder and<br> impose additional constraints on its layers.                                                                                                                                           | Input: 160x192x128<br>Outputs: output all 3 nested tumor subregions directly after the sigmoid<br><br><br>Using UNET based + VAE to segmentation                                                                                                                                                                                                    | - Add new VAE branch: using the auto-encoder branch is to add additional guidance and regularization to the encoder part, since the training dataset size is limited<br>- For normalization, we use Group Normalization (GN), which shows better than BatchNorm performance when batch size is small<br>- Ldice is a soft dice loss [19] applied to the decoder output ppred to match the segmentation mask ptrue<br>- We have also experimented with more sophisticated data augmentation techniques, including random histogram matching, affine image transforms, and random image filtering, which did not demonstrate any additional improvements.<br>- We have tried several data post-processing techniques to fine tune the segmentation predictions with CRF [14], but did not find it beneficial,<br>- Increasing the network depth further did not improve the performance, but increasing the network width (the number of features/filters) consistently improved the results                                                                                                          | - The size of model is too large, require 2 days of V100 32G to train with batchsize 1 (300 epochs)<br>-The additional VAE branch helped to regularize the shared encoder (in presence of limited data), which not only improved the performance, but helped to consistently achieve good training accuracy for any random initialization |
|                                        No New-Net                                       | - focus on the training process arguing that a well trained U-Net is hard to beat.<br>- incorporating additional measures such as region based training, additional training data, a simple postprocessing technique and a combination of loss functions-<br>- optimize the training procedure to maximize its performance<br>- It uses instance normalization [23] and leaky ReLU nonlinearities and reduces the number of feature maps before upsampling.<br>- using a soft Dice loss for the training of our network | UNET<br>- Input: 128x128x128<br>- Output: softmax or sigmoid                                                                                                                                                                                                                                                                                        | - With MRI intensity values being non standardized, normalization is critical to allow for data from different institutes, scanners and acquired with varying pro- tocols to be processed by one single algorithm<br>- We normalize each modality of each patient independently by subtracting the mean and dividing by the stan- dard deviation of the brain region. The region outside the brain is set to 0. As opposed to normalizing the entire image including the background, this strategy will yield comparative intensity values within the brain region irrespective of the size of the background region around it.<br>- Postprocessing: The BraTS challenge awards a Dice score of 1 if a label is absent in both the ground truth and the prediction. Conversely, only a single false positive voxel in a patient where no enhancing tumor is present in the ground truth will result in a Dice score of 0. Therefore we replace all enhancing tumor voxels with necrosis if the total number of predicted enhancing tumor is less than some threshold<br>- Disadvantage of Dice loss |                                                              One of the main challenges with brain tumor segmentation is the class imbalance in the dataset.<br>Small data, need to avoid overfitting<br>Could we use cascade model? Because there is one GT inside other GT                                                              |
| Ensembles of Densely-Connected CNNs with Label-Uncertainty for Brain Tumor Segmentation | - densely connected blocks of dilated convolutions are embedded in a shallow U-net-style structure of down/upsampling and skip connections.<br>- newly designed loss function which models label noise and uncer-tainty: Label-Uncertainty Loss and Focal Loss                                                                                                                                                                                                                                                          | Densenet-based Architecture, multitask for prediction<br>Input: the input tensor to the model has dimensions 2 * 4 * 5 * 192 * 192.                                                                                                                                                                                                                 | - Design new loss function for problem<br>- the nonzero intensities in the training, validation and testing sets were standardized, this being done across individual volumes rather than across the training set                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Use focal loss to handle imbalance data                                                                                                                                                                                                                                                                                                   |
| Learning Contextual and Attentive Information for Brain Tumor Segmentation              | - design multiple deep architectures<br>of varied structures to learning contextual and attentive information,                                                                                                                                                                                                                                                                                                                                                                                                          | decompose the multi-class brain tumor segmentation into three different but related sub-tasks to deal with the class - imbalance problem.<br>- Model Cascade:<br>- (1) Coarse segmenta- tion to detect whole tumor.<br>- (2) Refined segmentation for whole tumor and its intra-tumoral classes.<br>- (3) Precise segmentation for enhancing tumor. | Attention Mechanism using SE block, Cascade Network                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Using this Cascade model with other research                                                                                                                                                                                                                                                                                              |
